# LMM-Vibes Gradio Data Files Documentation

This document describes the data files expected by the LMM-Vibes Gradio visualization app, their purposes, and how they're used across different tabs.

## Overview

The Gradio app expects pipeline results in the **Functional Metrics format**, which consists of **4 required files**:

1. `model_cluster_scores.json` - Per model-cluster combination metrics
2. `cluster_scores.json` - Per cluster metrics (aggregated across all models)  
3. `model_scores.json` - Per model metrics (aggregated across all clusters)
4. `clustered_results.jsonl` - Raw conversation data with cluster assignments

## File Descriptions

### 1. `model_cluster_scores.json`

**Purpose**: Contains metrics for each model-cluster combination (e.g., "GPT-4 in cluster A")

**Generated by**: Functional metrics pipeline (`stringsight.metrics.functional_metrics`)

**Structure**:
```json
{
  "model_name": {
    "cluster_name": {
      "size": int,                    // Number of conversations for this model in this cluster
      "proportion": float,            // Fraction of model's conversations in this cluster (0-1)
      "quality": {                    // Raw quality scores for this model-cluster combination
        "metric_name": float          // e.g., "helpfulness": 4.2, "accuracy": 3.8
      },
      "quality_delta": {              // Relative quality: how much better/worse than model's overall average
        "metric_name": float          // e.g., "helpfulness": +0.3 (cluster is 0.3 points above model baseline)
      },
      "proportion_delta": float,      // Salience: how much this model over/under-represents vs average across all models
      "examples": [                   // Sample conversation IDs for this model-cluster
        [conversation_id, conversation_metadata, property_metadata], ...
      ],
      
      // Bootstrap confidence intervals (when enabled):
      "proportion_ci": {"lower": float, "upper": float, "mean": float},
      "quality_ci": {"metric_name": {"lower": float, "upper": float, "mean": float}},
      "quality_delta_ci": {"metric_name": {"lower": float, "upper": float, "mean": float}},
      "proportion_delta_ci": {"lower": float, "upper": float, "mean": float},
      
      // Significance testing (when bootstrap enabled):
      "quality_delta_significant": {"metric_name": bool},  // True if quality_delta CI doesn't contain 0
      "proportion_delta_significant": bool                 // True if proportion_delta CI doesn't contain 0
    }
  }
}
```

### 2. `cluster_scores.json`

**Purpose**: Contains metrics for each cluster aggregated across all models (e.g., "cluster A across all models")

**Generated by**: Functional metrics pipeline (`stringsight.metrics.functional_metrics`)

**Structure**:
```json
{
  "cluster_name": {
    "size": int,                      // Total conversations across all models in this cluster
    "proportion": float,              // Fraction of all conversations in this cluster
    "quality": {                      // Average quality scores across all models for this cluster
      "metric_name": float
    },
    "quality_delta": {                // How this cluster compares to overall average quality
      "metric_name": float
    },
    "examples": [...],                // Sample conversations from all models in this cluster
    
    // Bootstrap CIs (when enabled):
    "proportion_ci": {...},
    "quality_ci": {...},
    "quality_delta_ci": {...},
    
    // Significance testing (when bootstrap enabled):
    "quality_delta_significant": {"metric_name": bool}  // True if quality_delta CI doesn't contain 0
  }
}
```

### 3. `model_scores.json`

**Purpose**: Contains metrics for each model aggregated across all clusters (e.g., "GPT-4 across all clusters")

**Generated by**: Functional metrics pipeline (`stringsight.metrics.functional_metrics`)

**Structure**:
```json
{
  "model_name": {
    "size": int,                      // Total conversations for this model across all clusters
    "proportion": float,              // Always 1.0 (model represents 100% of its own conversations)
    "quality": {                      // Average quality scores for this model across all clusters
      "metric_name": float
    },
    "quality_delta": {                // How this model compares to cross-model average
      "metric_name": float
    },
    "examples": [...],                // Sample conversations for this model across all clusters
    
    // Bootstrap CIs (when enabled):
    "proportion_ci": {...},
    "quality_ci": {...},
    "quality_delta_ci": {...},
    
    // Significance testing (when bootstrap enabled):
    "quality_delta_significant": {"metric_name": bool}  // True if quality_delta CI doesn't contain 0
  }
}
```

### 4. `clustered_results.jsonl`

**Purpose**: Raw conversation data with cluster assignments (one JSON line per conversation)

**Generated by**: Clustering pipeline (`stringsight.clusterers`)

**Structure** (one line per conversation):
```json
{
  "id": "conversation_id",
  "prompt": "user prompt text",
  "model_response": "model response text",
  "model": "model_name",
  "cluster": "cluster_name",
  "property_description": "cluster description",
  "fine_cluster_id": int,           // Fine-grained cluster ID
  "coarse_cluster_id": int,         // Coarse-grained cluster ID (if hierarchical)
  "scores": {                       // Quality scores for this conversation
    "helpfulness": float,
    "accuracy": float,
    // ... other quality metrics
  },
  "metadata": {                     // Additional conversation metadata
    "timestamp": "2024-01-01T00:00:00Z",
    "dataset": "dataset_name",
    // ... other metadata
  }
}
```

> **Accepted alternative column names (for backward compatibility)**
>
> | Canonical column | Also accepted |
> |------------------|-------------------------------------------|
> | `prompt` | `question`, `input`, `user_prompt` |
> | `fine_cluster_id` | `property_description_fine_cluster_id` |
> | `coarse_cluster_id` | `property_description_coarse_cluster_id` |
> | `fine_cluster_label` | `property_description_fine_cluster_label` |

## Tab Usage by File

### üìä Overview Tab
**Files used**: `model_cluster_scores.json`, `cluster_scores.json`, `model_scores.json`

**Fields from `model_cluster_scores.json`**:
- `size` - Number of conversations per model-cluster
- `proportion` - What fraction of model's conversations are in this cluster
- `quality` - Raw quality scores for each metric
- `quality_delta` - Relative quality compared to model baseline
- `proportion_delta` - Salience (how much model over/under-represents)
- `proportion_delta_significant` - Statistical significance of salience
- `quality_delta_significant` - Statistical significance of quality deltas
- `proportion_ci` - Confidence intervals for proportion
- `quality_ci` - Confidence intervals for quality scores

**Fields from `cluster_scores.json`**:
- `size` - Total conversations in cluster across all models
- `proportion` - Fraction of all conversations in this cluster
- `quality` - Average quality scores for cluster

**Fields from `model_scores.json`**:
- `size` - Total conversations for each model (for battle count display)

### üìã View Clusters Tab
**Files used**: `clustered_results.jsonl`, `cluster_scores.json`

**Fields from `clustered_results.jsonl`**:
- `id` - Conversation ID
- `prompt` - User prompt text
- `model_response` - Model response text
- `model` - Model name
- `cluster` - Cluster name
- `property_description` - Cluster description  
- `fine_cluster_id` / `coarse_cluster_id` - Cluster IDs (alternatively `property_description_fine_cluster_id` / `property_description_coarse_cluster_id`)
- `scores` - Quality scores for individual conversations

**Fields from `cluster_scores.json`**:
- `size` - Total conversations in cluster across all models
- `proportion` - Fraction of all conversations in this cluster
- `quality` - Average quality scores for cluster
- `examples` - Sample conversation IDs

### üìã View Examples Tab
**Files used**: `clustered_results.jsonl`

**Fields from `clustered_results.jsonl`**:
- `id` - Conversation ID
- `prompt` - User prompt text
- `model_response` - Model response text
- `model` - Model name
- `cluster` - Cluster name
- `property_description` - Cluster description
- `scores` - Quality scores
- `metadata` - Additional conversation metadata
- `fine_cluster_id` / `coarse_cluster_id` - Cluster IDs (alt: `property_description_fine_cluster_id` / `property_description_coarse_cluster_id`)

### üìà Functional Metrics Tables Tab
**Files used**: `model_cluster_scores.json`, `cluster_scores.json`, `model_scores.json`

**Fields from `model_cluster_scores.json`** (Model-Cluster Scores table):
- `Model` - Model name
- `Cluster` - Cluster name
- `Size` - Number of conversations
- `Proportion (%)` - Percentage of model's conversations
- `Proportion Delta (%)` - Salience (deviation from average)
- `Quality_*` - Raw quality scores for each metric
- `Quality_Delta_*` - Relative quality scores
- `Proportion CI` - Confidence interval for proportion
- `Proportion Delta CI` - Confidence interval for salience
- `Proportion Delta Significant` - Statistical significance
- `Quality_Delta_*_Significant` - Statistical significance for each quality metric

**Fields from `cluster_scores.json`** (Cluster Scores table):
- `Cluster` - Cluster name
- `Size` - Total conversations across all models
- `Proportion (%)` - Percentage of all conversations
- `Quality_*` - Average quality scores across all models
- `Quality_Delta_*` - Relative quality compared to overall average
- `Proportion CI` - Confidence interval for proportion
- `Quality_*_CI` - Confidence intervals for each quality metric
- `Quality_Delta_*_CI` - Confidence intervals for quality deltas
- `Quality_Delta_*_Significant` - Statistical significance for each quality metric

**Fields from `model_scores.json`** (Model Scores table):
- `Model` - Model name
- `Size` - Total conversations for this model
- `Proportion (%)` - Always 100% (model represents all its conversations)
- `Quality_*` - Average quality scores across all clusters
- `Quality_Delta_*` - Relative quality compared to cross-model average
- `Proportion CI` - Confidence interval for proportion
- `Quality_*_CI` - Confidence intervals for each quality metric
- `Quality_Delta_*_CI` - Confidence intervals for quality deltas
- `Quality_Delta_*_Significant` - Statistical significance for each quality metric

### üìä Plots Tab
**Files used**: `model_cluster_scores.json` (converted to DataFrame)

**Fields used**:
- `model` - Model name
- `property` - Cluster name
- `size` - Number of conversations
- `proportion` - Fraction of model's conversations
- `quality_*` - Raw quality scores for each metric
- `quality_delta_*` - Relative quality scores
- `proportion_delta` - Salience metric
- `proportion_ci_lower` - Lower bound of proportion confidence interval (when bootstrap enabled)
- `proportion_ci_upper` - Upper bound of proportion confidence interval (when bootstrap enabled)

### üêõ Debug Data Tab
**Files used**: All files for data structure inspection

**Purpose**: Inspect the structure and content of all loaded data files to identify issues with data loading or formatting.

## Data Flow Summary

```
Pipeline Output Files ‚Üí Gradio App ‚Üí Tab-Specific Processing
‚îú‚îÄ‚îÄ model_cluster_scores.json ‚Üí Overview, Frequency, Plots tabs
‚îú‚îÄ‚îÄ cluster_scores.json ‚Üí Overview, Frequency tab (Cluster Scores table)
‚îú‚îÄ‚îÄ model_scores.json ‚Üí Overview, Frequency tab (Model Scores table)
‚îî‚îÄ‚îÄ clustered_results.jsonl ‚Üí Clusters, Examples, Debug tabs
```

## Key Concepts

- **Proportion**: What fraction of the parent set falls into this subset
- **Quality**: Raw quality scores (e.g., helpfulness, accuracy ratings)
- **Quality Delta**: Relative quality compared to baseline (positive = better than average)
- **Proportion Delta** (Salience): How much a model over/under-represents in a cluster
- **Bootstrap CIs**: Confidence intervals computed by resampling conversations
- **Significance**: Whether confidence intervals contain zero (indicating statistical significance)

## File Dependencies

- **`model_cluster_scores.json`**, **`cluster_scores.json`**, **`model_scores.json** are all generated by the same functional metrics pipeline
- **`clustered_results.jsonl`** is generated by the clustering pipeline and used as input to the functional metrics pipeline
- All files must be present for the Gradio app to function properly
- The app automatically detects and validates the presence of all required files 