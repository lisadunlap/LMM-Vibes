<div align="center">

# ğŸ§  LMM-Vibes
### *Extract, cluster, and analyze behavioral properties from Large Language Models*

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://badge.fury.io/py/lmmvibes.svg)](https://badge.fury.io/py/lmmvibes)
[![Downloads](https://pepy.tech/badge/lmmvibes)](https://pepy.tech/project/lmmvibes)

*Potential name options: Axiom, Tesseract, Quill, Mentat, Autopilot, TempCheck, explAIner, Marcel (i just like that name, no relation to anything this does)*

---

**ğŸ¯ Help you understand how different generative models behave by automatically extracting behavioral properties from their responses, grouping similar behaviors together, and quantifying how important these behaviors are.**

</div>

## ğŸš€ Installation

```bash
# Basic installation
pip install lmmvibes

# With optional dependencies
pip install lmmvibes[full]  # includes sentence-transformers, wandb
```

### ğŸ“‹ Requirements

- ğŸ Python 3.8+
- ğŸ”‘ OpenAI API key (set as `OPENAI_API_KEY` environment variable)
- ğŸ“Š Optional: Weights & Biases account for experiment tracking

## âš¡ Quick Start

```python
import pandas as pd
from lmmvibes import explain

# Your data with model responses
df = pd.DataFrame({
    "question_id": ["q1", "q2", "q3"],
    "prompt": ["What is machine learning?", "Explain quantum computing", "Write a poem about AI"],
    "model_a": ["gpt-4", "gpt-4", "gpt-4"],
    "model_b": ["claude-3", "claude-3", "claude-3"],
    "model_a_response": ["ML is a subset of AI...", "Quantum computing uses...", "In circuits of light..."],
    "model_b_response": ["Machine learning involves...", "QC leverages quantum...", "Silicon dreams awaken..."],
    "winner": ["model_a", "model_b", "tie"]
})

# Extract and cluster behavioral properties
clustered_df, model_stats = explain(
    df,
    method="side_by_side",
    min_cluster_size=10,
    output_dir="results/"
)
```

### ğŸ¨ Viewing Results in Streamlit
```bash
# View clusters, examples, and metrics
streamlit run lmmvibes/viz/pipeline_results_app.py -- --results_dir results/

# View your clustered results interactively
streamlit run lmmvibes/viz/interactive_app.py -- --dataset results/clustered_results.parquet
```

## ğŸ“Š What You Get

<table>
<tr>
<td width="50%">

### ğŸ“ˆ **`clustered_df`** 
Your original data plus:
- âœ¨ Extracted behavioral properties (`property_description`, `category`, `impact`, `type`)
- ğŸ¯ Cluster assignments (`property_description_fine_cluster_id`, `property_description_coarse_cluster_id`)
- ğŸ·ï¸ Cluster labels (`property_description_fine_cluster_label`, `property_description_coarse_cluster_label`)

</td>
<td width="50%">

### ğŸ“‹ **`model_stats`** 
Per-model behavioral analysis:
- ğŸ” Which behaviors each model shows most/least
- âš–ï¸ Relative scores compared to other models
- ğŸ’¡ Example responses for each behavior cluster

</td>
</tr>
</table>

## ğŸ­ Interactive Visualization

Explore your results with two specialized viewers:

<details>
<summary>ğŸ” <strong>Pipeline Results Viewer (Comprehensive Analysis)</strong></summary>

```bash
streamlit run lmmvibes/viz/pipeline_results_app.py -- --results_dir results/
```

**ğŸŒŸ Features:**
- ğŸ† Model performance leaderboards and rankings
- ğŸ—ºï¸ Interactive heatmaps comparing models across behavioral clusters  
- ğŸ“Š Score distribution analysis and statistics
- ğŸ‘ï¸ Example viewer with actual model responses
- âš”ï¸ Head-to-head model comparisons

</details>

<details>
<summary>ğŸ§© <strong>Cluster Explorer (Detailed Clustering Analysis)</strong></summary>

```bash
streamlit run lmmvibes/viz/interactive_app.py -- --dataset results/clustered_results.parquet
```

**ğŸŒŸ Features:**
- ğŸ—‚ï¸ Browse clusters and their hierarchical structure
- ğŸ”— Explore cluster properties and relationships
- ğŸ“ˆ View cluster statistics and metadata

</details>

> ğŸ’¡ **Both viewers provide complementary perspectives** - use the Pipeline Results Viewer for comprehensive model analysis and the Cluster Explorer for detailed clustering insights.

## ğŸ“ Input Data Requirements

LMM-Vibes supports two primary analysis methods with specific data requirements:

### ğŸ¤ Side-by-Side Comparisons
For comparing two models head-to-head (like Arena-style battles):

<details>
<summary>ğŸ“‹ <strong>Required & Optional Columns</strong></summary>

**ğŸ”´ Required columns:**
- `question_id` - Unique identifier for each conversation/question
- `prompt` (or `user_prompt`) - The question or prompt given to models
- `model_a` - Name of the first model
- `model_b` - Name of the second model  
- `model_a_response` - Response from the first model
- `model_b_response` - Response from the second model

**ğŸŸ¡ Optional columns:**
- `score` - Dictionary of metrics: `{"winner": "model_a", "rating": 4.5, "helpfulness": 0.8}`
- Any additional metadata columns (language, category, difficulty, etc.)

</details>

```python
df = pd.DataFrame({
    "question_id": ["q1", "q2"],
    "prompt": ["Question text"],
    "model_a": ["gpt-4", "gpt-4"],
    "model_b": ["claude-3", "claude-3"],
    "model_a_response": ["Response from model A"],
    "model_b_response": ["Response from model B"],
    "score": [{"winner": "model_a"}, {"winner": "model_b", "confidence": 0.9}],  # optional
    "language": ["en", "en"],  # optional metadata
})
```

### ğŸ¯ Single Model Analysis
For analyzing individual model responses:

<details>
<summary>ğŸ“‹ <strong>Required & Optional Columns</strong></summary>

**ğŸ”´ Required columns:**
- `question_id` - Unique identifier for each conversation/question
- `prompt` (or `user_prompt`) - The question or prompt given to the model
- `model` - Name of the model
- `model_response` (or `response`) - The model's response

**ğŸŸ¡ Optional columns:**
- `score` - Dictionary of metrics: `{"rating": 4.2, "accuracy": 0.85, "helpfulness": 0.9}`
- Any additional metadata columns

</details>

```python
df = pd.DataFrame({
    "question_id": ["q1", "q2"],
    "prompt": ["Question text"],
    "model": ["gpt-4", "gpt-4"],
    "model_response": ["Model response"],
    "score": [{"rating": 4.2, "accuracy": 0.8}, {"rating": 4.5, "accuracy": 0.9}],  # optional
    "category": ["reasoning", "creative"],  # optional metadata
})
```

## ğŸ”§ Pipeline Components

LMM-Vibes follows a 4-stage pipeline architecture:

```
ğŸ“¥ Data Input â†’ ğŸ” Property Extraction â†’ ğŸ”„ Post-processing â†’ ğŸ¯ Clustering â†’ ğŸ“Š Metrics & Analysis
```

<details>
<summary>ğŸ” <strong>1. Property Extraction</strong> (<code>lmmvibes.extractors</code>)</summary>

**ğŸ¯ Goal**: Identify specific behavioral properties from model responses using LLM analysis.

Takes each conversation and asks an LLM (like GPT-4) to extract behavioral properties such as:
- "Provides step-by-step reasoning"
- "Uses formal/informal tone" 
- "Includes creative examples"
- "Shows uncertainty appropriately"

**ğŸ› ï¸ Available extractors:**
- `OpenAIExtractor` - Uses OpenAI API (GPT models)
- `VLLMExtractor` - Uses local models via vLLM
- `BatchExtractor` - Makes a file for the batch API

</details>

<details>
<summary>ğŸ”„ <strong>2. Post-processing</strong> (<code>lmmvibes.postprocess</code>)</summary>

**ğŸ¯ Goal**: Parse and validate the extracted properties into structured data.

- `LLMJsonParser` - Converts raw LLM responses into structured property objects
- `PropertyValidator` - Ensures properties meet quality standards and required fields

</details>

<details>
<summary>ğŸ¯ <strong>3. Clustering</strong> (<code>lmmvibes.clusterers</code>)</summary>

**ğŸ¯ Goal**: Group similar behavioral properties into coherent clusters for analysis.

Takes individual properties like "explains step-by-step" and "shows work clearly" and groups them into clusters like "Reasoning Transparency". Creates both fine-grained and coarse-grained cluster hierarchies.

**ğŸ› ï¸ Available clusterers:**
- `HDBSCANClusterer` - Density-based clustering (recommended for >10k samples)
- `HierarchicalClusterer` - Traditional hierarchical clustering with LLM-powered naming

</details>

<details>
<summary>ğŸ“Š <strong>4. Metrics & Analysis</strong> (<code>lmmvibes.metrics</code>)</summary>

**ğŸ¯ Goal**: Calculate model performance statistics and behavioral rankings.

Computes which models excel at which behavioral patterns:
- Model scores per behavior cluster
- Relative strengths/weaknesses between models  
- Statistical significance of differences
- Example responses for each cluster

**ğŸ› ï¸ Available metrics:**
- `SideBySideMetrics` - For model comparison data
- `SingleModelMetrics` - For individual model analysis

</details>

## âš™ï¸ Configuration Options

```python
clustered_df, model_stats = explain(
    df,
    method="side_by_side",              # or "single_model"
    system_prompt="one_sided_system_prompt",  # custom extraction prompt
    min_cluster_size=30,                # minimum cluster size
    embedding_model="openai",           # or any sentence-transformer model
    hierarchical=True,                  # create coarse clusters
    output_dir="results/",              # save outputs here
    use_wandb=True,                     # log to Weights & Biases
    wandb_project="my-lmm-analysis"
)
```

## ğŸ“ Output Files

When you specify `output_dir`, LMM-Vibes automatically saves:

| File | Description |
|------|-------------|
| ğŸ“Š `clustered_results.parquet` | Full dataset with properties and clusters |
| ğŸ“‹ `full_dataset.json` | Complete dataset in JSON format |
| ğŸ“ˆ `model_stats.json` | Per-model behavioral statistics |
| ğŸ“ `summary.txt` | Human-readable analysis summary |

## ğŸ’¡ Examples

### ğŸ† Analyzing Model Competition Data
```python
# Load your Arena-style competition data
df = pd.read_parquet("arena_data.parquet")

# Extract behavioral differences
clustered_df, stats = explain(df, method="side_by_side")

# See which behaviors each model excels at
for model, model_stats in stats.items():
    print(f"\n{model} excels at:")
    for behavior in model_stats["fine"][:3]:  # top 3 behaviors
        print(f"  â€¢ {behavior.property_description} (score: {behavior.score:.2f})")
```

### ğŸ¯ Understanding Model Capabilities
```python
# Single model analysis
df = pd.DataFrame({
    "question_id": range(100),
    "prompt": ["Explain quantum physics"] * 100,
    "model": ["gpt-4"] * 100,
    "response": responses  # your model responses
})

clustered_df, stats = explain(df, method="single_model")
```

## ğŸ”¬ Advanced: Running Pipeline Components

<details>
<summary>ğŸ› ï¸ <strong>For more control, you can run each pipeline stage separately:</strong></summary>

```python
from lmmvibes.core import PropertyDataset
from lmmvibes.extractors import OpenAIExtractor
from lmmvibes.postprocess import LLMJsonParser, PropertyValidator
from lmmvibes.clusterers import HDBSCANClusterer
from lmmvibes.metrics import SideBySideMetrics

# 1. Load your data
dataset = PropertyDataset.from_dataframe(df, method="side_by_side")

# 2. Extract properties
extractor = OpenAIExtractor(
    system_prompt="one_sided_system_prompt",
    model="gpt-4o-mini",
    temperature=0.6
)
dataset = extractor(dataset)

# 3. Parse and validate properties
parser = LLMJsonParser()
dataset = parser(dataset)

validator = PropertyValidator()
dataset = validator(dataset)

# 4. Cluster properties
clusterer = HDBSCANClusterer(
    min_cluster_size=30,
    embedding_model="openai",
    hierarchical=True
)
dataset = clusterer(dataset)

# 5. Compute metrics
metrics = SideBySideMetrics()
dataset = metrics(dataset)

# 6. Save results
dataset.save("results/full_pipeline_output.json")
```

</details>

## ğŸ¤ Contributing

So uh, I'm still building this out a lot so maybe contribute when i have something more stable... but hey if you really wanna submit a PR i'll review it. 

If you want to know more about the nitty gritty abstractions and code structure, check out the [design doc](README_ABSTRACTION.md).

---

<div align="center">

**â“ Need help?** Check out the [documentation](https://lmm-vibes.readthedocs.io) or open an issue on GitHub. 

*(JK this doesnt exist claude assumes i am more organized than i actually am)*

---

**â­ Star this repo if you find it useful!** | **ğŸ› Report bugs** | **ğŸ’¡ Request features**

</div>