{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to LMM-Vibes","text":"<p>LMM-Vibes is a comprehensive framework for working with Large Language Models (LLMs) and evaluating their performance across various tasks and datasets.</p>"},{"location":"#what-is-lmm-vibes","title":"What is LMM-Vibes?","text":"<p>LMM-Vibes provides tools and utilities for:</p> <ul> <li>Model Evaluation: Comprehensive evaluation metrics and benchmarks</li> <li>Data Processing: Tools for handling conversation data and model outputs</li> <li>Visualization: Interactive dashboards and analysis tools</li> <li>Research Support: Utilities for reproducible research workflows</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install the package\npip install -e .\n\n# Run a basic evaluation\npython -m lmmvibes.evaluate --model your-model --dataset your-data\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\ude80 Easy Setup: Simple installation and configuration</li> <li>\ud83d\udcca Rich Metrics: Comprehensive evaluation metrics</li> <li>\ud83c\udfaf Flexible: Support for various model types and datasets</li> <li>\ud83d\udcc8 Visualization: Built-in plotting and analysis tools</li> <li>\ud83d\udd2c Research Ready: Designed for reproducible research</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out our Installation Guide to get up and running quickly, or dive into the User Guide for detailed usage instructions.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! See our Contributing Guide for details on how to get involved.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started: Installation and setup</li> <li>User Guide: How to use LMM-Vibes</li> <li>API Reference: Detailed API documentation</li> <li>Development: Contributing guidelines</li> </ul>"},{"location":"api/core/","title":"Core API Reference","text":"<p>Reference documentation for the core LMM-Vibes functions and classes.</p>"},{"location":"api/core/#evaluation-functions","title":"Evaluation Functions","text":""},{"location":"api/core/#evaluate_model","title":"<code>evaluate_model</code>","text":"<p>Main function for evaluating model performance.</p> <pre><code>from lmmvibes.evaluation import evaluate_model\n\nevaluate_model(\n    data: List[Dict],\n    metrics: List[str] = [\"accuracy\"],\n    config: Optional[Dict] = None,\n    model_name: Optional[str] = None\n) -&gt; Dict\n</code></pre> <p>Parameters: - <code>data</code>: List of dictionaries containing evaluation data - <code>metrics</code>: List of metric names to compute - <code>config</code>: Optional configuration dictionary - <code>model_name</code>: Optional name for the model being evaluated</p> <p>Returns: - Dictionary containing evaluation results</p> <p>Example:</p> <pre><code>data = [\n    {\"question\": \"What is 2+2?\", \"answer\": \"4\", \"model_output\": \"The answer is 4.\"}\n]\n\nresults = evaluate_model(\n    data=data,\n    metrics=[\"accuracy\", \"bleu\", \"rouge\"],\n    model_name=\"gpt-4\"\n)\n</code></pre>"},{"location":"api/core/#batch_evaluate","title":"<code>batch_evaluate</code>","text":"<p>Evaluate multiple models or datasets.</p> <pre><code>from lmmvibes.evaluation import batch_evaluate\n\nbatch_evaluate(\n    data: List[Dict],\n    models: List[str],\n    metrics: List[str] = [\"accuracy\"],\n    config: Optional[Dict] = None\n) -&gt; Dict[str, Dict]\n</code></pre> <p>Parameters: - <code>data</code>: List of dictionaries containing evaluation data - <code>models</code>: List of model names to evaluate - <code>metrics</code>: List of metric names to compute - <code>config</code>: Optional configuration dictionary</p> <p>Returns: - Dictionary mapping model names to their evaluation results</p>"},{"location":"api/core/#data-functions","title":"Data Functions","text":""},{"location":"api/core/#load_dataset","title":"<code>load_dataset</code>","text":"<p>Load data from various file formats.</p> <pre><code>from lmmvibes.data import load_dataset\n\nload_dataset(\n    file_path: str,\n    format: Optional[str] = None,\n    validation: bool = True\n) -&gt; List[Dict]\n</code></pre> <p>Parameters: - <code>file_path</code>: Path to the data file - <code>format</code>: File format (auto-detected if None) - <code>validation</code>: Whether to validate data format</p> <p>Returns: - List of dictionaries containing the loaded data</p> <p>Supported Formats: - JSONL (<code>.jsonl</code>) - JSON (<code>.json</code>) - CSV (<code>.csv</code>)</p>"},{"location":"api/core/#save_dataset","title":"<code>save_dataset</code>","text":"<p>Save data to various file formats.</p> <pre><code>from lmmvibes.data import save_dataset\n\nsave_dataset(\n    data: List[Dict],\n    file_path: str,\n    format: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Parameters: - <code>data</code>: List of dictionaries to save - <code>file_path</code>: Path where to save the file - <code>format</code>: File format (auto-detected if None)</p>"},{"location":"api/core/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api/core/#evaluationconfig","title":"<code>EvaluationConfig</code>","text":"<p>Configuration class for evaluation settings.</p> <pre><code>from lmmvibes.config import EvaluationConfig\n\nconfig = EvaluationConfig(\n    metrics: List[str] = [\"accuracy\"],\n    batch_size: int = 32,\n    save_results: bool = False,\n    output_dir: str = \"./results\"\n)\n</code></pre> <p>Attributes: - <code>metrics</code>: List of metrics to compute - <code>batch_size</code>: Batch size for processing - <code>save_results</code>: Whether to save results to disk - <code>output_dir</code>: Directory to save results</p>"},{"location":"api/core/#dataconfig","title":"<code>DataConfig</code>","text":"<p>Configuration class for data settings.</p> <pre><code>from lmmvibes.config import DataConfig\n\nconfig = DataConfig(\n    input_format: str = \"jsonl\",\n    output_format: str = \"json\",\n    validation: bool = True,\n    required_fields: List[str] = [\"question\", \"answer\", \"model_output\"]\n)\n</code></pre> <p>Attributes: - <code>input_format</code>: Format of input data - <code>output_format</code>: Format for output data - <code>validation</code>: Whether to validate data - <code>required_fields</code>: Required fields in data</p>"},{"location":"api/core/#utility-functions","title":"Utility Functions","text":""},{"location":"api/core/#save_results","title":"<code>save_results</code>","text":"<p>Save evaluation results to file.</p> <pre><code>from lmmvibes.utils import save_results\n\nsave_results(\n    results: Dict,\n    file_path: str,\n    format: str = \"json\"\n) -&gt; None\n</code></pre> <p>Parameters: - <code>results</code>: Evaluation results dictionary - <code>file_path</code>: Path where to save results - <code>format</code>: Output format (\"json\", \"yaml\", \"csv\")</p>"},{"location":"api/core/#load_results","title":"<code>load_results</code>","text":"<p>Load evaluation results from file.</p> <pre><code>from lmmvibes.utils import load_results\n\nload_results(file_path: str) -&gt; Dict\n</code></pre> <p>Parameters: - <code>file_path</code>: Path to the results file</p> <p>Returns: - Dictionary containing the loaded results</p>"},{"location":"api/core/#visualization-functions","title":"Visualization Functions","text":""},{"location":"api/core/#plot_metrics","title":"<code>plot_metrics</code>","text":"<p>Create plots of evaluation metrics.</p> <pre><code>from lmmvibes.visualization import plot_metrics\n\nplot_metrics(\n    results: Dict,\n    metrics: Optional[List[str]] = None,\n    title: Optional[str] = None,\n    save_path: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Parameters: - <code>results</code>: Evaluation results dictionary - <code>metrics</code>: Specific metrics to plot (all if None) - <code>title</code>: Plot title - <code>save_path</code>: Path to save the plot</p>"},{"location":"api/core/#plot_comparison","title":"<code>plot_comparison</code>","text":"<p>Compare multiple models or results.</p> <pre><code>from lmmvibes.visualization import plot_comparison\n\nplot_comparison(\n    results_list: List[Dict],\n    labels: Optional[List[str]] = None,\n    metrics: Optional[List[str]] = None,\n    title: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Parameters: - <code>results_list</code>: List of evaluation results - <code>labels</code>: Labels for each result set - <code>metrics</code>: Specific metrics to compare - <code>title</code>: Plot title</p>"},{"location":"api/core/#metric-classes","title":"Metric Classes","text":""},{"location":"api/core/#metric","title":"<code>Metric</code>","text":"<p>Base class for custom metrics.</p> <pre><code>from lmmvibes.metrics import Metric\n\nclass CustomMetric(Metric):\n    def compute(self, predictions: List[str], references: List[str]) -&gt; float:\n        # Your custom computation logic\n        return score\n</code></pre> <p>Methods: - <code>compute(predictions, references)</code>: Compute the metric score</p>"},{"location":"api/core/#built-in-metrics","title":"Built-in Metrics","text":"<p>Available built-in metrics:</p> <ul> <li><code>accuracy</code>: Exact match accuracy</li> <li><code>bleu</code>: BLEU score for text similarity</li> <li><code>rouge</code>: ROUGE score for text similarity</li> <li><code>bert_score</code>: BERT-based semantic similarity</li> <li><code>exact_match</code>: Exact string matching</li> <li><code>f1_score</code>: F1 score for classification tasks</li> </ul>"},{"location":"api/core/#error-classes","title":"Error Classes","text":""},{"location":"api/core/#evaluationerror","title":"<code>EvaluationError</code>","text":"<p>Raised when evaluation fails.</p> <pre><code>from lmmvibes.exceptions import EvaluationError\n\ntry:\n    results = evaluate_model(data=data)\nexcept EvaluationError as e:\n    print(f\"Evaluation failed: {e}\")\n</code></pre>"},{"location":"api/core/#datavalidationerror","title":"<code>DataValidationError</code>","text":"<p>Raised when data validation fails.</p> <pre><code>from lmmvibes.exceptions import DataValidationError\n\ntry:\n    data = load_dataset(\"data.jsonl\")\nexcept DataValidationError as e:\n    print(f\"Data validation failed: {e}\")\n</code></pre>"},{"location":"api/core/#next-steps","title":"Next Steps","text":"<ul> <li>Check out Utilities for additional helper functions</li> <li>Learn about Basic Usage for practical examples</li> <li>Explore Configuration for advanced setup </li> </ul>"},{"location":"api/utilities/","title":"Utilities API Reference","text":"<p>Reference documentation for utility functions and helper classes in LMM-Vibes.</p>"},{"location":"api/utilities/#data-utilities","title":"Data Utilities","text":""},{"location":"api/utilities/#validate_data_format","title":"<code>validate_data_format</code>","text":"<p>Validate that data follows the expected format.</p> <pre><code>from lmmvibes.utils import validate_data_format\n\nvalidate_data_format(\n    data: List[Dict],\n    required_fields: List[str] = [\"question\", \"answer\", \"model_output\"],\n    optional_fields: List[str] = []\n) -&gt; bool\n</code></pre> <p>Parameters: - <code>data</code>: List of data dictionaries to validate - <code>required_fields</code>: Fields that must be present in each item - <code>optional_fields</code>: Fields that may be present</p> <p>Returns: - <code>True</code> if data is valid, raises <code>DataValidationError</code> otherwise</p>"},{"location":"api/utilities/#convert_data_format","title":"<code>convert_data_format</code>","text":"<p>Convert data between different formats.</p> <pre><code>from lmmvibes.utils import convert_data_format\n\nconvert_data_format(\n    data: List[Dict],\n    target_format: str,\n    field_mapping: Optional[Dict[str, str]] = None\n) -&gt; List[Dict]\n</code></pre> <p>Parameters: - <code>data</code>: Input data - <code>target_format</code>: Target format (\"jsonl\", \"json\", \"csv\") - <code>field_mapping</code>: Optional mapping of field names</p> <p>Returns: - Data in the target format</p>"},{"location":"api/utilities/#file-utilities","title":"File Utilities","text":""},{"location":"api/utilities/#ensure_directory","title":"<code>ensure_directory</code>","text":"<p>Ensure a directory exists, creating it if necessary.</p> <pre><code>from lmmvibes.utils import ensure_directory\n\nensure_directory(path: str) -&gt; None\n</code></pre> <p>Parameters: - <code>path</code>: Directory path to ensure exists</p>"},{"location":"api/utilities/#get_file_extension","title":"<code>get_file_extension</code>","text":"<p>Get the file extension from a path.</p> <pre><code>from lmmvibes.utils import get_file_extension\n\nget_file_extension(file_path: str) -&gt; str\n</code></pre> <p>Parameters: - <code>file_path</code>: Path to the file</p> <p>Returns: - File extension (e.g., \".json\", \".csv\")</p>"},{"location":"api/utilities/#sanitize_filename","title":"<code>sanitize_filename</code>","text":"<p>Create a safe filename from a string.</p> <pre><code>from lmmvibes.utils import sanitize_filename\n\nsanitize_filename(filename: str) -&gt; str\n</code></pre> <p>Parameters: - <code>filename</code>: Original filename</p> <p>Returns: - Sanitized filename safe for filesystem</p>"},{"location":"api/utilities/#text-utilities","title":"Text Utilities","text":""},{"location":"api/utilities/#normalize_text","title":"<code>normalize_text</code>","text":"<p>Normalize text for consistent processing.</p> <pre><code>from lmmvibes.utils import normalize_text\n\nnormalize_text(\n    text: str,\n    lowercase: bool = True,\n    remove_punctuation: bool = False,\n    remove_whitespace: bool = False\n) -&gt; str\n</code></pre> <p>Parameters: - <code>text</code>: Text to normalize - <code>lowercase</code>: Whether to convert to lowercase - <code>remove_punctuation</code>: Whether to remove punctuation - <code>remove_whitespace</code>: Whether to normalize whitespace</p> <p>Returns: - Normalized text</p>"},{"location":"api/utilities/#tokenize_text","title":"<code>tokenize_text</code>","text":"<p>Tokenize text into words or subwords.</p> <pre><code>from lmmvibes.utils import tokenize_text\n\ntokenize_text(\n    text: str,\n    method: str = \"word\",\n    language: str = \"en\"\n) -&gt; List[str]\n</code></pre> <p>Parameters: - <code>text</code>: Text to tokenize - <code>method</code>: Tokenization method (\"word\", \"subword\", \"sentence\") - <code>language</code>: Language code for tokenization</p> <p>Returns: - List of tokens</p>"},{"location":"api/utilities/#metric-utilities","title":"Metric Utilities","text":""},{"location":"api/utilities/#compute_metric","title":"<code>compute_metric</code>","text":"<p>Compute a single metric on predictions and references.</p> <pre><code>from lmmvibes.utils import compute_metric\n\ncompute_metric(\n    metric_name: str,\n    predictions: List[str],\n    references: List[str],\n    **kwargs\n) -&gt; float\n</code></pre> <p>Parameters: - <code>metric_name</code>: Name of the metric to compute - <code>predictions</code>: List of model predictions - <code>references</code>: List of reference answers - <code>**kwargs</code>: Additional arguments for the metric</p> <p>Returns: - Computed metric score</p>"},{"location":"api/utilities/#aggregate_metrics","title":"<code>aggregate_metrics</code>","text":"<p>Aggregate multiple metric scores.</p> <pre><code>from lmmvibes.utils import aggregate_metrics\n\naggregate_metrics(\n    scores: List[float],\n    method: str = \"mean\"\n) -&gt; float\n</code></pre> <p>Parameters: - <code>scores</code>: List of metric scores - <code>method</code>: Aggregation method (\"mean\", \"median\", \"max\", \"min\")</p> <p>Returns: - Aggregated score</p>"},{"location":"api/utilities/#configuration-utilities","title":"Configuration Utilities","text":""},{"location":"api/utilities/#load_config_file","title":"<code>load_config_file</code>","text":"<p>Load configuration from a file.</p> <pre><code>from lmmvibes.utils import load_config_file\n\nload_config_file(\n    file_path: str,\n    validate: bool = True\n) -&gt; Dict\n</code></pre> <p>Parameters: - <code>file_path</code>: Path to configuration file - <code>validate</code>: Whether to validate the configuration</p> <p>Returns: - Configuration dictionary</p>"},{"location":"api/utilities/#save_config_file","title":"<code>save_config_file</code>","text":"<p>Save configuration to a file.</p> <pre><code>from lmmvibes.utils import save_config_file\n\nsave_config_file(\n    config: Dict,\n    file_path: str,\n    format: str = \"yaml\"\n) -&gt; None\n</code></pre> <p>Parameters: - <code>config</code>: Configuration dictionary - <code>file_path</code>: Path where to save the configuration - <code>format</code>: Output format (\"yaml\", \"json\")</p>"},{"location":"api/utilities/#logging-utilities","title":"Logging Utilities","text":""},{"location":"api/utilities/#setup_logging","title":"<code>setup_logging</code>","text":"<p>Set up logging configuration.</p> <pre><code>from lmmvibes.utils import setup_logging\n\nsetup_logging(\n    level: str = \"INFO\",\n    file_path: Optional[str] = None,\n    format_string: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Parameters: - <code>level</code>: Logging level - <code>file_path</code>: Optional log file path - <code>format_string</code>: Optional log format string</p>"},{"location":"api/utilities/#get_logger","title":"<code>get_logger</code>","text":"<p>Get a logger instance.</p> <pre><code>from lmmvibes.utils import get_logger\n\nget_logger(name: str = \"lmmvibes\") -&gt; logging.Logger\n</code></pre> <p>Parameters: - <code>name</code>: Logger name</p> <p>Returns: - Logger instance</p>"},{"location":"api/utilities/#time-utilities","title":"Time Utilities","text":""},{"location":"api/utilities/#timer","title":"<code>Timer</code>","text":"<p>Context manager for timing operations.</p> <pre><code>from lmmvibes.utils import Timer\n\nwith Timer(\"operation_name\"):\n    # Your code here\n    pass\n</code></pre>"},{"location":"api/utilities/#format_duration","title":"<code>format_duration</code>","text":"<p>Format a duration in human-readable format.</p> <pre><code>from lmmvibes.utils import format_duration\n\nformat_duration(seconds: float) -&gt; str\n</code></pre> <p>Parameters: - <code>seconds</code>: Duration in seconds</p> <p>Returns: - Formatted duration string (e.g., \"2m 30s\")</p>"},{"location":"api/utilities/#progress-utilities","title":"Progress Utilities","text":""},{"location":"api/utilities/#progressbar","title":"<code>ProgressBar</code>","text":"<p>Simple progress bar for long-running operations.</p> <pre><code>from lmmvibes.utils import ProgressBar\n\nwith ProgressBar(total=100, desc=\"Processing\") as pbar:\n    for i in range(100):\n        # Your processing code\n        pbar.update(1)\n</code></pre> <p>Parameters: - <code>total</code>: Total number of items - <code>desc</code>: Description of the operation</p>"},{"location":"api/utilities/#validation-utilities","title":"Validation Utilities","text":""},{"location":"api/utilities/#validate_metric_name","title":"<code>validate_metric_name</code>","text":"<p>Validate that a metric name is supported.</p> <pre><code>from lmmvibes.utils import validate_metric_name\n\nvalidate_metric_name(metric_name: str) -&gt; bool\n</code></pre> <p>Parameters: - <code>metric_name</code>: Name of the metric to validate</p> <p>Returns: - <code>True</code> if metric is supported, <code>False</code> otherwise</p>"},{"location":"api/utilities/#validate_file_format","title":"<code>validate_file_format</code>","text":"<p>Validate that a file format is supported.</p> <pre><code>from lmmvibes.utils import validate_file_format\n\nvalidate_file_format(format_name: str) -&gt; bool\n</code></pre> <p>Parameters: - <code>format_name</code>: Name of the file format to validate</p> <p>Returns: - <code>True</code> if format is supported, <code>False</code> otherwise</p>"},{"location":"api/utilities/#error-handling-utilities","title":"Error Handling Utilities","text":""},{"location":"api/utilities/#handle_errors","title":"<code>handle_errors</code>","text":"<p>Decorator for consistent error handling.</p> <pre><code>from lmmvibes.utils import handle_errors\n\n@handle_errors\ndef my_function():\n    # Your function code\n    pass\n</code></pre>"},{"location":"api/utilities/#retry_on_failure","title":"<code>retry_on_failure</code>","text":"<p>Decorator for retrying failed operations.</p> <pre><code>from lmmvibes.utils import retry_on_failure\n\n@retry_on_failure(max_attempts=3, delay=1.0)\ndef my_function():\n    # Your function code\n    pass\n</code></pre> <p>Parameters: - <code>max_attempts</code>: Maximum number of retry attempts - <code>delay</code>: Delay between retries in seconds</p>"},{"location":"api/utilities/#next-steps","title":"Next Steps","text":"<ul> <li>Check out Core API for main functions and classes</li> <li>Learn about Basic Usage for practical examples</li> <li>Explore Configuration for advanced setup </li> </ul>"},{"location":"development/contributing/","title":"Contributing to LMM-Vibes","text":"<p>Thank you for your interest in contributing to LMM-Vibes! This guide will help you get started.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>Git</li> <li>Basic knowledge of Python and machine learning</li> </ul>"},{"location":"development/contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<ol> <li> <p>Fork the Repository <code>bash    # Fork on GitHub, then clone your fork    git clone https://github.com/your-username/LMM-Vibes.git    cd LMM-Vibes</code></p> </li> <li> <p>Install Development Dependencies    ```bash    # Install in development mode    pip install -e .</p> </li> </ol> <p># Install development dependencies    pip install -r requirements-dev.txt    ```</p> <ol> <li>Set Up Pre-commit Hooks <code>bash    # Install pre-commit hooks    pre-commit install</code></li> </ol>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<pre><code># Create and switch to a new branch\ngit checkout -b feature/your-feature-name\n\n# Or for bug fixes\ngit checkout -b fix/your-bug-description\n</code></pre>"},{"location":"development/contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Write your code following the Code Style guidelines</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> </ul>"},{"location":"development/contributing/#3-test-your-changes","title":"3. Test Your Changes","text":"<pre><code># Run the test suite\npytest\n\n# Run with coverage\npytest --cov=lmmvibes\n\n# Run linting\nflake8 lmmvibes/\nblack lmmvibes/\n</code></pre>"},{"location":"development/contributing/#4-commit-your-changes","title":"4. Commit Your Changes","text":"<pre><code># Stage your changes\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"feat: add new evaluation metric\"\n\n# Push to your fork\ngit push origin feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#5-create-a-pull-request","title":"5. Create a Pull Request","text":"<ol> <li>Go to your fork on GitHub</li> <li>Click \"New Pull Request\"</li> <li>Select your feature branch</li> <li>Fill out the PR template</li> <li>Submit the PR</li> </ol>"},{"location":"development/contributing/#code-style","title":"Code Style","text":""},{"location":"development/contributing/#python-code","title":"Python Code","text":"<p>We follow PEP 8 with some modifications:</p> <ul> <li>Line Length: 88 characters (Black default)</li> <li>Docstrings: Google style</li> <li>Type Hints: Required for all public functions</li> </ul>"},{"location":"development/contributing/#example","title":"Example","text":"<pre><code>from typing import List, Dict, Optional\n\ndef evaluate_model(\n    data: List[Dict],\n    metrics: List[str] = [\"accuracy\"],\n    config: Optional[Dict] = None\n) -&gt; Dict:\n    \"\"\"Evaluate model performance on given data.\n\n    Args:\n        data: List of dictionaries containing evaluation data\n        metrics: List of metric names to compute\n        config: Optional configuration dictionary\n\n    Returns:\n        Dictionary containing evaluation results\n\n    Raises:\n        EvaluationError: If evaluation fails\n    \"\"\"\n    # Your implementation here\n    pass\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<ul> <li>All public functions must have docstrings</li> <li>Use Google style docstrings</li> <li>Include type hints</li> <li>Add examples for complex functions</li> </ul>"},{"location":"development/contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new functionality</li> <li>Aim for at least 80% code coverage</li> <li>Use descriptive test names</li> <li>Test both success and failure cases</li> </ul>"},{"location":"development/contributing/#example-test","title":"Example Test","text":"<pre><code>import pytest\nfrom lmmvibes.evaluation import evaluate_model\n\ndef test_evaluate_model_basic():\n    \"\"\"Test basic model evaluation functionality.\"\"\"\n    data = [\n        {\"question\": \"What is 2+2?\", \"answer\": \"4\", \"model_output\": \"4\"}\n    ]\n\n    results = evaluate_model(data, metrics=[\"accuracy\"])\n\n    assert \"accuracy\" in results\n    assert results[\"accuracy\"] == 1.0\n\ndef test_evaluate_model_invalid_data():\n    \"\"\"Test evaluation with invalid data.\"\"\"\n    with pytest.raises(ValueError):\n        evaluate_model([])\n</code></pre>"},{"location":"development/contributing/#project-structure","title":"Project Structure","text":"<pre><code>lmmvibes/\n\u251c\u2500\u2500 lmmvibes/           # Main package\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 evaluation.py   # Core evaluation functions\n\u2502   \u251c\u2500\u2500 data.py         # Data loading and processing\n\u2502   \u251c\u2500\u2500 metrics.py      # Evaluation metrics\n\u2502   \u251c\u2500\u2500 visualization.py # Plotting and visualization\n\u2502   \u251c\u2500\u2500 config.py       # Configuration management\n\u2502   \u2514\u2500\u2500 utils.py        # Utility functions\n\u251c\u2500\u2500 tests/              # Test suite\n\u251c\u2500\u2500 docs/               # Documentation\n\u251c\u2500\u2500 examples/           # Example scripts\n\u2514\u2500\u2500 requirements.txt    # Dependencies\n</code></pre>"},{"location":"development/contributing/#adding-new-features","title":"Adding New Features","text":""},{"location":"development/contributing/#1-new-metrics","title":"1. New Metrics","text":"<p>To add a new evaluation metric:</p> <ol> <li>Create the metric class in <code>lmmvibes/metrics.py</code></li> <li>Inherit from the <code>Metric</code> base class</li> <li>Implement the <code>compute</code> method</li> <li>Add tests in <code>tests/test_metrics.py</code></li> <li>Update documentation</li> </ol>"},{"location":"development/contributing/#2-new-data-formats","title":"2. New Data Formats","text":"<p>To add support for new data formats:</p> <ol> <li>Add format detection in <code>lmmvibes/data.py</code></li> <li>Implement loading/saving functions</li> <li>Add validation logic</li> <li>Write tests</li> <li>Update documentation</li> </ol>"},{"location":"development/contributing/#3-new-visualization-types","title":"3. New Visualization Types","text":"<p>To add new visualization types:</p> <ol> <li>Add plotting functions in <code>lmmvibes/visualization.py</code></li> <li>Follow the existing API patterns</li> <li>Add configuration options</li> <li>Write tests</li> <li>Update documentation</li> </ol>"},{"location":"development/contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting bugs, please include:</p> <ol> <li>Environment: Python version, OS, package versions</li> <li>Reproduction: Steps to reproduce the issue</li> <li>Expected vs Actual: What you expected vs what happened</li> <li>Error Messages: Full error traceback</li> <li>Minimal Example: Code that reproduces the issue</li> </ol>"},{"location":"development/contributing/#feature-requests","title":"Feature Requests","text":"<p>When requesting features, please include:</p> <ol> <li>Use Case: What problem does this solve?</li> <li>Proposed Solution: How should it work?</li> <li>Alternatives: What other approaches have you considered?</li> <li>Implementation: Any thoughts on implementation?</li> </ol>"},{"location":"development/contributing/#code-review-process","title":"Code Review Process","text":"<ol> <li>Automated Checks: All PRs must pass CI checks</li> <li>Review: At least one maintainer must approve</li> <li>Tests: All tests must pass</li> <li>Documentation: Documentation must be updated</li> <li>Style: Code must follow style guidelines</li> </ol>"},{"location":"development/contributing/#release-process","title":"Release Process","text":""},{"location":"development/contributing/#for-maintainers","title":"For Maintainers","text":"<ol> <li>Update Version: Update version in <code>setup.py</code></li> <li>Update Changelog: Add release notes</li> <li>Create Release: Tag and create GitHub release</li> <li>Publish: Upload to PyPI</li> </ol>"},{"location":"development/contributing/#version-numbers","title":"Version Numbers","text":"<p>We use semantic versioning (MAJOR.MINOR.PATCH):</p> <ul> <li>MAJOR: Breaking changes</li> <li>MINOR: New features (backward compatible)</li> <li>PATCH: Bug fixes (backward compatible)</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: Use GitHub issues for bugs and feature requests</li> <li>Discussions: Use GitHub discussions for questions</li> <li>Documentation: Check the docs first</li> <li>Code: Look at existing code for examples</li> </ul>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors will be recognized in:</p> <ul> <li>GitHub contributors list</li> <li>Release notes</li> <li>Documentation acknowledgments</li> </ul>"},{"location":"development/contributing/#next-steps","title":"Next Steps","text":"<ul> <li>Check out the Testing Guide for detailed testing information</li> <li>Read the API Reference to understand the codebase</li> <li>Look at Basic Usage for usage examples </li> </ul>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>Comprehensive guide to testing in LMM-Vibes.</p>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-execution","title":"Basic Test Execution","text":"<pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test file\npytest tests/test_evaluation.py\n\n# Run specific test function\npytest tests/test_evaluation.py::test_evaluate_model_basic\n</code></pre>"},{"location":"development/testing/#test-coverage","title":"Test Coverage","text":"<pre><code># Run with coverage report\npytest --cov=lmmvibes\n\n# Generate HTML coverage report\npytest --cov=lmmvibes --cov-report=html\n\n# Generate XML coverage report (for CI)\npytest --cov=lmmvibes --cov-report=xml\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":"<pre><code># Run unit tests only\npytest -m \"not integration\"\n\n# Run integration tests only\npytest -m integration\n\n# Run slow tests\npytest -m slow\n\n# Skip slow tests\npytest -m \"not slow\"\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>import pytest\nfrom lmmvibes.evaluation import evaluate_model\n\nclass TestEvaluation:\n    \"\"\"Test suite for evaluation functionality.\"\"\"\n\n    def test_basic_evaluation(self):\n        \"\"\"Test basic model evaluation.\"\"\"\n        # Arrange\n        data = [{\"question\": \"What is 2+2?\", \"answer\": \"4\", \"model_output\": \"4\"}]\n\n        # Act\n        results = evaluate_model(data, metrics=[\"accuracy\"])\n\n        # Assert\n        assert \"accuracy\" in results\n        assert results[\"accuracy\"] == 1.0\n\n    def test_empty_data(self):\n        \"\"\"Test evaluation with empty data.\"\"\"\n        with pytest.raises(ValueError, match=\"Data cannot be empty\"):\n            evaluate_model([])\n\n    @pytest.mark.parametrize(\"metric\", [\"accuracy\", \"bleu\", \"rouge\"])\n    def test_metric_computation(self, metric):\n        \"\"\"Test computation of different metrics.\"\"\"\n        data = [{\"question\": \"Test\", \"answer\": \"answer\", \"model_output\": \"answer\"}]\n        results = evaluate_model(data, metrics=[metric])\n        assert metric in results\n</code></pre>"},{"location":"development/testing/#test-fixtures","title":"Test Fixtures","text":"<pre><code>import pytest\n\n@pytest.fixture\ndef sample_data():\n    \"\"\"Provide sample data for tests.\"\"\"\n    return [\n        {\"question\": \"What is 2+2?\", \"answer\": \"4\", \"model_output\": \"4\"},\n        {\"question\": \"What is 3+3?\", \"answer\": \"6\", \"model_output\": \"6\"}\n    ]\n\n@pytest.fixture\ndef evaluation_config():\n    \"\"\"Provide evaluation configuration.\"\"\"\n    return {\n        \"metrics\": [\"accuracy\", \"bleu\"],\n        \"batch_size\": 32,\n        \"save_results\": False\n    }\n\ndef test_evaluation_with_fixtures(sample_data, evaluation_config):\n    \"\"\"Test evaluation using fixtures.\"\"\"\n    results = evaluate_model(sample_data, config=evaluation_config)\n    assert \"accuracy\" in results\n    assert \"bleu\" in results\n</code></pre>"},{"location":"development/testing/#mocking","title":"Mocking","text":"<pre><code>from unittest.mock import patch, MagicMock\n\ndef test_external_api_call():\n    \"\"\"Test function that calls external API.\"\"\"\n    with patch('lmmvibes.external_api.call_api') as mock_api:\n        mock_api.return_value = {\"result\": \"success\"}\n\n        # Your test code here\n        result = call_external_function()\n\n        assert result == \"success\"\n        mock_api.assert_called_once()\n</code></pre>"},{"location":"development/testing/#test-categories_1","title":"Test Categories","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Test individual functions and classes in isolation.</p> <pre><code>def test_metric_computation():\n    \"\"\"Test metric computation logic.\"\"\"\n    from lmmvibes.metrics import AccuracyMetric\n\n    metric = AccuracyMetric()\n    predictions = [\"4\", \"6\", \"8\"]\n    references = [\"4\", \"6\", \"8\"]\n\n    score = metric.compute(predictions, references)\n    assert score == 1.0\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Test interactions between components.</p> <pre><code>@pytest.mark.integration\ndef test_full_evaluation_pipeline():\n    \"\"\"Test complete evaluation pipeline.\"\"\"\n    # Load data\n    data = load_test_dataset()\n\n    # Run evaluation\n    results = evaluate_model(data, metrics=[\"accuracy\", \"bleu\"])\n\n    # Save results\n    save_results(results, \"test_results.json\")\n\n    # Load and verify\n    loaded_results = load_results(\"test_results.json\")\n    assert loaded_results == results\n</code></pre>"},{"location":"development/testing/#performance-tests","title":"Performance Tests","text":"<p>Test performance characteristics.</p> <pre><code>@pytest.mark.slow\ndef test_large_dataset_performance():\n    \"\"\"Test performance with large dataset.\"\"\"\n    import time\n\n    # Generate large dataset\n    large_data = generate_test_data(10000)\n\n    start_time = time.time()\n    results = evaluate_model(large_data, metrics=[\"accuracy\"])\n    end_time = time.time()\n\n    # Should complete within reasonable time\n    assert end_time - start_time &lt; 60  # 60 seconds\n</code></pre>"},{"location":"development/testing/#test-data","title":"Test Data","text":""},{"location":"development/testing/#creating-test-data","title":"Creating Test Data","text":"<pre><code>def generate_test_data(num_samples: int = 100) -&gt; List[Dict]:\n    \"\"\"Generate synthetic test data.\"\"\"\n    import random\n\n    questions = [\n        \"What is 2+2?\",\n        \"What is the capital of France?\",\n        \"Explain gravity\",\n        \"What is photosynthesis?\"\n    ]\n\n    data = []\n    for i in range(num_samples):\n        question = random.choice(questions)\n        answer = f\"Answer {i}\"\n        model_output = f\"Model output {i}\"\n\n        data.append({\n            \"question\": question,\n            \"answer\": answer,\n            \"model_output\": model_output,\n            \"metadata\": {\"id\": i}\n        })\n\n    return data\n</code></pre>"},{"location":"development/testing/#test-data-files","title":"Test Data Files","text":"<p>Store test data in <code>tests/data/</code>:</p> <pre><code>tests/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 sample.jsonl\n\u2502   \u251c\u2500\u2500 large_dataset.jsonl\n\u2502   \u2514\u2500\u2500 edge_cases.jsonl\n\u251c\u2500\u2500 test_evaluation.py\n\u2514\u2500\u2500 test_metrics.py\n</code></pre>"},{"location":"development/testing/#assertions-and-checks","title":"Assertions and Checks","text":""},{"location":"development/testing/#basic-assertions","title":"Basic Assertions","text":"<pre><code>def test_basic_assertions():\n    \"\"\"Test basic assertion patterns.\"\"\"\n    results = evaluate_model(sample_data)\n\n    # Check key exists\n    assert \"accuracy\" in results\n\n    # Check value range\n    assert 0.0 &lt;= results[\"accuracy\"] &lt;= 1.0\n\n    # Check type\n    assert isinstance(results[\"accuracy\"], float)\n\n    # Check approximate equality\n    assert results[\"accuracy\"] == pytest.approx(0.85, rel=0.01)\n</code></pre>"},{"location":"development/testing/#custom-assertions","title":"Custom Assertions","text":"<pre><code>def assert_valid_results(results: Dict):\n    \"\"\"Custom assertion for result validation.\"\"\"\n    required_keys = [\"accuracy\", \"bleu\", \"rouge\"]\n\n    for key in required_keys:\n        assert key in results, f\"Missing key: {key}\"\n        assert isinstance(results[key], (int, float)), f\"Invalid type for {key}\"\n        assert 0.0 &lt;= results[key] &lt;= 1.0, f\"Value out of range for {key}\"\n\ndef test_results_validation():\n    \"\"\"Test custom result validation.\"\"\"\n    results = evaluate_model(sample_data)\n    assert_valid_results(results)\n</code></pre>"},{"location":"development/testing/#error-testing","title":"Error Testing","text":""},{"location":"development/testing/#testing-exceptions","title":"Testing Exceptions","text":"<pre><code>def test_invalid_input():\n    \"\"\"Test handling of invalid input.\"\"\"\n    with pytest.raises(ValueError, match=\"Data cannot be empty\"):\n        evaluate_model([])\n\n    with pytest.raises(TypeError):\n        evaluate_model(\"not a list\")\n\n    with pytest.raises(KeyError):\n        evaluate_model([{\"invalid\": \"data\"}])\n</code></pre>"},{"location":"development/testing/#testing-warnings","title":"Testing Warnings","text":"<pre><code>import warnings\n\ndef test_deprecation_warning():\n    \"\"\"Test deprecation warnings.\"\"\"\n    with pytest.warns(DeprecationWarning, match=\"deprecated\"):\n        deprecated_function()\n</code></pre>"},{"location":"development/testing/#test-configuration","title":"Test Configuration","text":""},{"location":"development/testing/#pytestini","title":"pytest.ini","text":"<pre><code>[tool:pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts = -v --tb=short\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: marks tests as integration tests\n    unit: marks tests as unit tests\n</code></pre>"},{"location":"development/testing/#conftestpy","title":"conftest.py","text":"<pre><code>import pytest\nimport tempfile\nimport os\n\n@pytest.fixture(scope=\"session\")\ndef temp_dir():\n    \"\"\"Create temporary directory for tests.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield tmpdir\n\n@pytest.fixture(autouse=True)\ndef setup_test_environment():\n    \"\"\"Set up test environment.\"\"\"\n    # Set test environment variables\n    os.environ[\"LMMVIBES_TESTING\"] = \"true\"\n    yield\n    # Cleanup\n    if \"LMMVIBES_TESTING\" in os.environ:\n        del os.environ[\"LMMVIBES_TESTING\"]\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: Tests\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.8, 3.9, 3.10]\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v3\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        pip install -e .\n        pip install -r requirements-dev.txt\n\n    - name: Run tests\n      run: |\n        pytest --cov=lmmvibes --cov-report=xml\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":"<ol> <li>Test Naming: Use descriptive test names</li> <li>Test Isolation: Each test should be independent</li> <li>Fast Tests: Keep unit tests fast (&lt; 1 second)</li> <li>Coverage: Aim for high code coverage</li> <li>Documentation: Document complex test scenarios</li> <li>Maintenance: Keep tests up to date with code changes</li> </ol>"},{"location":"development/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Check out Contributing for development guidelines</li> <li>Read the API Reference to understand the codebase</li> <li>Look at Basic Usage for usage examples </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install LMM-Vibes and its dependencies.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/installation/#option-1-install-from-source-recommended","title":"Option 1: Install from Source (Recommended)","text":"<pre><code># Clone the repository\ngit clone https://github.com/your-username/LMM-Vibes.git\ncd LMM-Vibes\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#option-2-install-dependencies-only","title":"Option 2: Install Dependencies Only","text":"<pre><code># Install required packages\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>To verify your installation, run:</p> <pre><code>python -c \"import lmmvibes; print('LMM-Vibes installed successfully!')\"\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<ol> <li>Import Error: Make sure you're in the correct directory and have installed the package</li> <li>Missing Dependencies: Run <code>pip install -r requirements.txt</code></li> <li>Version Conflicts: Consider using a virtual environment</li> </ol>"},{"location":"getting-started/installation/#using-a-virtual-environment","title":"Using a Virtual Environment","text":"<pre><code># Create virtual environment\npython -m venv lmmvibes-env\n\n# Activate (Linux/Mac)\nsource lmmvibes-env/bin/activate\n\n# Activate (Windows)\nlmmvibes-env\\Scripts\\activate\n\n# Install package\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installed, check out the Quick Start Guide to begin using LMM-Vibes. </p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get up and running with LMM-Vibes in minutes.</p>"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":"<pre><code>pip install lmm-vibes\n# or for development\npip install -e .\n</code></pre> <p>Set up your OpenAI API key:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quick-start/#1-import-lmm-vibes","title":"1. Import LMM-Vibes","text":"<pre><code>import pandas as pd\nfrom lmmvibes import explain\n</code></pre>"},{"location":"getting-started/quick-start/#2-prepare-your-data","title":"2. Prepare Your Data","text":"<p>Side-by-side comparison format:</p> <pre><code>df = pd.DataFrame([\n    {\n        \"question_id\": \"q1\",\n        \"model_a\": \"gpt-4\", \n        \"model_b\": \"claude-3\",\n        \"model_a_response\": \"The answer is 4.\",\n        \"model_b_response\": \"2 + 2 equals 4.\",\n        \"winner\": \"tie\"  # optional\n    }\n])\n</code></pre> <p>Single model format:</p> <pre><code>df = pd.DataFrame([\n    {\n        \"question_id\": \"q1\",\n        \"model\": \"gpt-4\",\n        \"model_response\": \"The answer is 4.\",\n        \"score\": 8.5  # optional\n    }\n])\n</code></pre>"},{"location":"getting-started/quick-start/#3-analyze-model-behavior","title":"3. Analyze Model Behavior","text":"<pre><code># Side-by-side comparison\nclustered_df, model_stats = explain(\n    df,\n    method=\"side_by_side\",\n    min_cluster_size=30,\n    output_dir=\"results/\"\n)\n\n# Single model analysis  \nclustered_df, model_stats = explain(\n    df,\n    method=\"single_model\",\n    min_cluster_size=20,\n    output_dir=\"results/\"\n)\n</code></pre>"},{"location":"getting-started/quick-start/#4-explore-results","title":"4. Explore Results","text":"<pre><code># View behavioral clusters\nprint(clustered_df[['property_description', 'property_description_coarse_cluster_label']].head())\n\n# Check model rankings\nprint(model_stats)\n</code></pre>"},{"location":"getting-started/quick-start/#5-interactive-visualization","title":"5. Interactive Visualization","text":"<pre><code>streamlit run lmmvibes/viz/interactive_app.py -- --dataset results/clustered_results.parquet\n</code></pre>"},{"location":"getting-started/quick-start/#advanced-usage","title":"Advanced Usage","text":""},{"location":"getting-started/quick-start/#custom-prompts","title":"Custom Prompts","text":"<pre><code>custom_prompt = \"\"\"\nAnalyze this conversation and identify the key behavioral difference.\nFocus on: reasoning style, factual accuracy, helpfulness.\n\"\"\"\n\nclustered_df, model_stats = explain(\n    df,\n    method=\"side_by_side\", \n    system_prompt=custom_prompt,\n    model_name=\"gpt-4o\",\n    temperature=0.1\n)\n</code></pre>"},{"location":"getting-started/quick-start/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<pre><code>clustered_df, model_stats = explain(\n    df,\n    method=\"side_by_side\",\n    hierarchical=True,  # Creates both fine and coarse clusters\n    max_coarse_clusters=15,\n    embedding_model=\"openai\"  # Higher quality embeddings\n)\n</code></pre>"},{"location":"getting-started/quick-start/#pipeline-configuration","title":"Pipeline Configuration","text":"<pre><code>from lmmvibes.pipeline import PipelineBuilder\nfrom lmmvibes.extractors import OpenAIExtractor\nfrom lmmvibes.clusterers import HDBSCANClusterer\n\npipeline = (PipelineBuilder(\"Custom Pipeline\")\n    .extract_properties(OpenAIExtractor(model=\"gpt-4o-mini\"))\n    .cluster_properties(HDBSCANClusterer(min_cluster_size=15))\n    .configure(use_wandb=True)\n    .build())\n\n# Use custom pipeline\nclustered_df, model_stats = explain(\n    df, \n    custom_pipeline=pipeline\n)\n</code></pre>"},{"location":"getting-started/quick-start/#data-formats","title":"Data Formats","text":""},{"location":"getting-started/quick-start/#required-columns","title":"Required Columns","text":"<p>Side-by-side: - <code>question_id</code> - Unique identifier for each question - <code>model_a</code>, <code>model_b</code> - Model names - <code>model_a_response</code>, <code>model_b_response</code> - Model responses - <code>winner</code> (optional) - \"model_a\", \"model_b\", or \"tie\"</p> <p>Single model: - <code>question_id</code> - Unique identifier  - <code>model</code> - Model name - <code>model_response</code> - Model response - <code>score</code> (optional) - Numeric quality score</p>"},{"location":"getting-started/quick-start/#output-files","title":"Output Files","text":"<p>When you specify <code>output_dir</code>, LMM-Vibes saves: - <code>clustered_results.parquet</code> - Complete results with clusters - <code>model_stats.json</code> - Model performance statistics - <code>full_dataset.json</code> - Complete dataset for reuse - <code>summary.txt</code> - Human-readable summary</p>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about the explain() and label() functions</li> <li>Understand the output files</li> <li>Explore the API Reference </li> </ul>"},{"location":"user-guide/basic-usage/","title":"Explain and Label Functions","text":"<p>Learn how to use the two main functions in LMM-Vibes for analyzing model behavior.</p>"},{"location":"user-guide/basic-usage/#core-functions","title":"Core Functions","text":"<p>LMM-Vibes provides two primary functions:</p> <ul> <li><code>explain()</code>: Discovers behavioral patterns through clustering </li> <li><code>label()</code>: Classifies behavior using predefined taxonomies</li> </ul> <p>Both functions analyze conversation data and return clustered results with model statistics.</p>"},{"location":"user-guide/basic-usage/#the-explain-function","title":"The <code>explain()</code> Function","text":"<p>The <code>explain()</code> function automatically discovers behavioral patterns in model responses through property extraction and clustering.</p>"},{"location":"user-guide/basic-usage/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom lmmvibes import explain\n\n# Load your conversation data\ndf = pd.read_csv(\"model_conversations.csv\")\n\n# Analyze side-by-side comparisons\nclustered_df, model_stats = explain(\n    df,\n    method=\"side_by_side\",\n    min_cluster_size=30,\n    output_dir=\"results/\"\n)\n\n# Analyze single model responses\nclustered_df, model_stats = explain(\n    df,\n    method=\"single_model\", \n    min_cluster_size=20,\n    output_dir=\"results/\"\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#parameters","title":"Parameters","text":"<p>Core Parameters: - <code>df</code>: Input DataFrame with conversation data - <code>method</code>: <code>\"side_by_side\"</code> or <code>\"single_model\"</code> - <code>system_prompt</code>: Custom prompt for property extraction (optional) - <code>output_dir</code>: Directory to save results</p> <p>Extraction Parameters: - <code>model_name</code>: LLM for property extraction (default: <code>\"gpt-4o\"</code>) - <code>temperature</code>: Temperature for LLM calls (default: <code>0.7</code>) - <code>max_workers</code>: Parallel workers for API calls (default: <code>16</code>)</p> <p>Clustering Parameters: - <code>clusterer</code>: Clustering method (<code>\"hdbscan\"</code>, <code>\"hierarchical\"</code>) - <code>min_cluster_size</code>: Minimum cluster size (default: <code>30</code>) - <code>embedding_model</code>: <code>\"openai\"</code> or sentence-transformer model - <code>hierarchical</code>: Create both fine and coarse clusters (default: <code>False</code>)</p>"},{"location":"user-guide/basic-usage/#examples","title":"Examples","text":"<p>Custom System Prompt:</p> <pre><code>custom_prompt = \"\"\"\nAnalyze this conversation and identify behavioral differences.\nFocus on: reasoning approach, factual accuracy, response style.\nReturn a JSON object with 'property_description' and 'property_evidence'.\n\"\"\"\n\nclustered_df, model_stats = explain(\n    df,\n    method=\"side_by_side\",\n    system_prompt=custom_prompt\n)\n</code></pre> <p>Hierarchical Clustering:</p> <pre><code>clustered_df, model_stats = explain(\n    df,\n    method=\"side_by_side\",\n    hierarchical=True,\n    max_coarse_clusters=15,\n    min_cluster_size=20\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#the-label-function","title":"The <code>label()</code> Function","text":"<p>The <code>label()</code> function classifies model behavior using a predefined taxonomy rather than discovering patterns.</p>"},{"location":"user-guide/basic-usage/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from lmmvibes import label\n\n# Define your evaluation taxonomy\ntaxonomy = {\n    \"accuracy\": \"Is the response factually correct?\",\n    \"helpfulness\": \"Does the response address the user's needs?\", \n    \"clarity\": \"Is the response clear and well-structured?\",\n    \"safety\": \"Does the response avoid harmful content?\"\n}\n\n# Classify responses\nclustered_df, model_stats = label(\n    df,\n    taxonomy=taxonomy,\n    model_name=\"gpt-4o-mini\",\n    output_dir=\"results/\"\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#parameters_1","title":"Parameters","text":"<p>Core Parameters: - <code>df</code>: Input DataFrame (must be single-model format) - <code>taxonomy</code>: Dictionary mapping labels to descriptions - <code>model_name</code>: LLM for classification (default: <code>\"gpt-4o-mini\"</code>) - <code>output_dir</code>: Directory to save results</p> <p>Other Parameters: - <code>temperature</code>: Temperature for classification (default: <code>0.0</code>) - <code>max_workers</code>: Parallel workers (default: <code>8</code>) - <code>verbose</code>: Print progress information (default: <code>True</code>)</p>"},{"location":"user-guide/basic-usage/#example","title":"Example","text":"<p>Quality Assessment:</p> <pre><code>quality_taxonomy = {\n    \"excellent\": \"Response is comprehensive, accurate, and well-structured\",\n    \"good\": \"Response is mostly accurate with minor issues\",\n    \"fair\": \"Response has some accuracy or clarity problems\", \n    \"poor\": \"Response has significant issues or inaccuracies\"\n}\n\nclustered_df, model_stats = label(\n    df,\n    taxonomy=quality_taxonomy,\n    temperature=0.0,  # Deterministic classification\n    output_dir=\"quality_results/\"\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#data-formats","title":"Data Formats","text":""},{"location":"user-guide/basic-usage/#side-by-side-format-for-explain-only","title":"Side-by-side Format (for <code>explain()</code> only)","text":"<pre><code>df = pd.DataFrame([\n    {\n        \"question_id\": \"q1\",\n        \"model_a\": \"gpt-4\",\n        \"model_b\": \"claude-3\", \n        \"model_a_response\": \"Response from model A...\",\n        \"model_b_response\": \"Response from model B...\",\n        \"winner\": \"tie\"  # optional: \"model_a\", \"model_b\", or \"tie\"\n    }\n])\n</code></pre>"},{"location":"user-guide/basic-usage/#single-model-format-for-both-functions","title":"Single Model Format (for both functions)","text":"<pre><code>df = pd.DataFrame([\n    {\n        \"question_id\": \"q1\",\n        \"model\": \"gpt-4\",\n        \"model_response\": \"The model's response...\",\n        \"score\": 8.5  # optional: numeric quality score\n    }\n])\n</code></pre>"},{"location":"user-guide/basic-usage/#understanding-results","title":"Understanding Results","text":""},{"location":"user-guide/basic-usage/#output-dataframes","title":"Output DataFrames","text":"<p>Both functions return a DataFrame with added columns:</p> <pre><code># Original columns plus:\nprint(clustered_df.columns)\n# ['question_id', 'model', 'model_response', \n#  'property_description', 'property_evidence',\n#  'property_description_cluster_id', 'property_description_cluster_label']\n</code></pre>"},{"location":"user-guide/basic-usage/#model-statistics","title":"Model Statistics","text":"<pre><code>print(model_stats.keys())\n# Contains performance metrics, cluster distributions, and rankings\n</code></pre>"},{"location":"user-guide/basic-usage/#saved-files","title":"Saved Files","text":"<p>When <code>output_dir</code> is specified, both functions save: - <code>clustered_results.parquet</code> - Complete results with clusters - <code>model_stats.json</code> - Model performance statistics - <code>full_dataset.json</code> - Complete dataset for reanalysis - <code>summary.txt</code> - Human-readable summary</p>"},{"location":"user-guide/basic-usage/#when-to-use-each-function","title":"When to Use Each Function","text":"<p>Use <code>explain()</code> when: - You want to discover unknown behavioral patterns - You're comparing multiple models - You need flexible, data-driven analysis - You want to understand what makes models different</p> <p>Use <code>label()</code> when: - You have specific criteria to evaluate - You need consistent scoring across datasets - You're building evaluation pipelines - You want controlled, taxonomy-based analysis</p>"},{"location":"user-guide/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Understand the output files in detail</li> <li>Explore configuration options</li> <li>Learn about the pipeline architecture </li> </ul>"},{"location":"user-guide/configuration/","title":"Output Files","text":"<p>Understanding the files generated by LMM-Vibes analysis.</p> <p>When you run <code>explain()</code> or <code>label()</code> with an <code>output_dir</code>, LMM-Vibes saves comprehensive results across multiple file formats. This guide explains each output file and how to use them.</p>"},{"location":"user-guide/configuration/#core-output-files","title":"Core Output Files","text":""},{"location":"user-guide/configuration/#1-clustered_resultsparquet","title":"1. <code>clustered_results.parquet</code>","text":"<p>Primary results file with all analysis data</p> <p>This is the main output file containing your original data enriched with extracted properties and cluster assignments.</p> <pre><code>import pandas as pd\n\n# Load complete results\ndf = pd.read_parquet(\"results/clustered_results.parquet\")\n\n# Key columns added by LMM-Vibes:\nprint(df.columns)\n# ['question_id', 'model', 'model_response',           # Original data\n#  'property_description', 'property_evidence',       # Extracted properties  \n#  'property_description_cluster_id',                  # Cluster assignments\n#  'property_description_cluster_label',               # Human-readable cluster names\n#  'property_description_coarse_cluster_id',           # Hierarchical clusters (if enabled)\n#  'property_description_coarse_cluster_label']        # Coarse cluster names\n\n# Example: Find all responses in a specific cluster\ncluster_data = df[df['property_description_cluster_label'] == 'Detailed Technical Explanations']\n</code></pre> <p>Use this file for: - Interactive analysis and visualization - Building custom dashboards - Statistical analysis of results - Feeding into downstream ML pipelines</p>"},{"location":"user-guide/configuration/#2-model_statsjson","title":"2. <code>model_stats.json</code>","text":"<p>Model performance metrics and rankings</p> <p>Contains statistical summaries and performance metrics for each model analyzed.</p> <pre><code>import json\n\n# Load model statistics\nwith open(\"results/model_stats.json\", \"r\") as f:\n    model_stats = json.load(f)\n\n# Structure depends on your analysis:\nprint(model_stats.keys())\n\n# For functional metrics format:\nfunctional_metrics = model_stats[\"functional_metrics\"]\nmodel_scores = functional_metrics[\"model_scores\"]\ncluster_scores = functional_metrics[\"cluster_scores\"]\n\n# Example: Get quality scores for each model\nfor model_name, model_data in model_scores.items():\n    quality_scores = model_data[\"quality\"]\n    print(f\"{model_name}: {quality_scores}\")\n</code></pre> <p>Use this file for: - Model leaderboards and rankings - Performance comparisons - Quality assessment reports - Automated model selection</p>"},{"location":"user-guide/configuration/#3-full_datasetjson","title":"3. <code>full_dataset.json</code>","text":"<p>Complete dataset for reanalysis and caching</p> <p>Contains the entire <code>PropertyDataset</code> object with all conversations, properties, clusters, and metadata.</p> <pre><code>from lmmvibes.core.data_objects import PropertyDataset\n\n# Load complete dataset\ndataset = PropertyDataset.load(\"results/full_dataset.json\")\n\n# Access all components:\nprint(f\"Conversations: {len(dataset.conversations)}\")\nprint(f\"Properties: {len(dataset.properties)}\")  \nprint(f\"Clusters: {len(dataset.clusters)}\")\nprint(f\"Models: {dataset.all_models}\")\n\n# Rerun metrics with different parameters\nfrom lmmvibes import compute_metrics_only\nclustered_df, new_stats = compute_metrics_only(\n    \"results/full_dataset.json\",\n    method=\"single_model\",\n    output_dir=\"results_updated/\"\n)\n</code></pre> <p>Use this file for: - Recomputing metrics without re-extracting properties - Debugging and troubleshooting - Building analysis pipelines - Sharing complete analysis state</p>"},{"location":"user-guide/configuration/#additional-output-files","title":"Additional Output Files","text":""},{"location":"user-guide/configuration/#processing-stage-files","title":"Processing Stage Files","text":"<p>Property Extraction: - <code>raw_properties.jsonl</code> - Raw LLM responses before parsing - <code>extraction_stats.json</code> - API call statistics and timing - <code>extraction_samples.jsonl</code> - Sample inputs/outputs for debugging</p> <p>JSON Parsing: - <code>parsed_properties.jsonl</code> - Successfully parsed property objects - <code>parsing_stats.json</code> - Parsing success/failure statistics - <code>parsing_failures.jsonl</code> - Failed parsing attempts for debugging</p> <p>Validation: - <code>validated_properties.jsonl</code> - Properties that passed validation - <code>validation_stats.json</code> - Validation statistics</p> <p>Clustering: - <code>embeddings.parquet</code> - Property embeddings (if <code>include_embeddings=True</code>) - <code>clustered_results_lightweight.jsonl</code> - Results without embeddings - <code>summary_table.jsonl</code> - Cluster summary statistics</p> <p>Metrics: - <code>model_cluster_scores.json</code> - Per model-cluster performance - <code>cluster_scores.json</code> - Aggregate cluster metrics - <code>model_scores.json</code> - Aggregate model metrics</p>"},{"location":"user-guide/configuration/#summary-files","title":"Summary Files","text":"<p><code>summary.txt</code> - Human-readable analysis summary</p> <pre><code>LMM-Vibes Results Summary\n==================================================\n\nTotal conversations: 1,234\nTotal properties: 4,567  \nModels analyzed: 8\nFine clusters: 23\nCoarse clusters: 8\n\nModel Rankings (by average quality score):\n  1. gpt-4: 0.847\n  2. claude-3: 0.832\n  3. gemini-pro: 0.801\n  ...\n</code></pre>"},{"location":"user-guide/configuration/#working-with-output-files","title":"Working with Output Files","text":""},{"location":"user-guide/configuration/#loading-results-for-analysis","title":"Loading Results for Analysis","text":"<pre><code>import pandas as pd\nimport json\n\n# Quick analysis workflow\ndf = pd.read_parquet(\"results/clustered_results.parquet\")\nwith open(\"results/model_stats.json\") as f:\n    stats = json.load(f)\n\n# Analyze cluster distributions\ncluster_counts = df['property_description_cluster_label'].value_counts()\nprint(\"Top behavioral patterns:\")\nprint(cluster_counts.head(10))\n\n# Compare models within specific clusters  \nfor cluster in cluster_counts.head(5).index:\n    cluster_data = df[df['property_description_cluster_label'] == cluster]\n    model_dist = cluster_data['model'].value_counts()\n    print(f\"\\n{cluster}:\")\n    print(model_dist)\n</code></pre>"},{"location":"user-guide/configuration/#rerunning-analysis","title":"Rerunning Analysis","text":"<pre><code>from lmmvibes import compute_metrics_only\n\n# Recompute metrics with different parameters\nclustered_df, model_stats = compute_metrics_only(\n    input_path=\"results/full_dataset.json\",\n    method=\"single_model\", \n    metrics_kwargs={\n        'compute_confidence_intervals': True,\n        'bootstrap_samples': 1000\n    },\n    output_dir=\"results_with_ci/\"\n)\n</code></pre>"},{"location":"user-guide/configuration/#building-dashboards","title":"Building Dashboards","text":"<pre><code># Interactive visualization\nimport streamlit as st\n\n# Load results\n@st.cache_data\ndef load_data():\n    return pd.read_parquet(\"results/clustered_results.parquet\")\n\ndf = load_data()\n\n# Build interactive filters\nselected_models = st.multiselect(\"Select Models\", df['model'].unique())\nselected_clusters = st.multiselect(\"Select Clusters\", df['property_description_cluster_label'].unique())\n\n# Filter and display\nfiltered_df = df[\n    (df['model'].isin(selected_models)) &amp; \n    (df['property_description_cluster_label'].isin(selected_clusters))\n]\nst.dataframe(filtered_df)\n</code></pre>"},{"location":"user-guide/configuration/#file-format-details","title":"File Format Details","text":""},{"location":"user-guide/configuration/#parquet-vs-json-vs-jsonl","title":"Parquet vs JSON vs JSONL","text":"<p>Parquet (<code>.parquet</code>) - Binary format, fastest loading - Preserves data types  - Best for analysis and large datasets - Use: <code>pd.read_parquet()</code></p> <p>JSON (<code>.json</code>) - Human-readable structure - Good for configuration and metadata - Use: <code>json.load()</code></p> <p>JSONL (<code>.jsonl</code>) - Newline-delimited JSON - Streamable for large datasets - Each line is a JSON object - Use: <code>pd.read_json(..., lines=True)</code></p>"},{"location":"user-guide/configuration/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/#1-file-organization","title":"1. File Organization","text":"<pre><code>results/\n\u251c\u2500\u2500 clustered_results.parquet      # Primary analysis file\n\u251c\u2500\u2500 model_stats.json               # Model rankings  \n\u251c\u2500\u2500 full_dataset.json              # Complete state\n\u251c\u2500\u2500 summary.txt                    # Human summary\n\u251c\u2500\u2500 embeddings.parquet             # Embeddings (optional)\n\u2514\u2500\u2500 stage_outputs/                 # Detailed processing files\n    \u251c\u2500\u2500 parsed_properties.jsonl\n    \u251c\u2500\u2500 validation_stats.json\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"user-guide/configuration/#2-version-control","title":"2. Version Control","text":"<ul> <li>Include <code>summary.txt</code> and <code>model_stats.json</code> in version control</li> <li>Use <code>.gitignore</code> for large binary files like embeddings</li> <li>Tag important analysis runs</li> </ul>"},{"location":"user-guide/configuration/#3-reproducibility","title":"3. Reproducibility","text":"<ul> <li>Save the exact command/parameters used</li> <li>Keep <code>full_dataset.json</code> for reanalysis</li> <li>Document any post-processing steps</li> </ul>"},{"location":"user-guide/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Use the quickstart guide to generate these files</li> <li>Learn about explain() and label() functions  </li> <li>Explore visualization options </li> </ul>"}]}